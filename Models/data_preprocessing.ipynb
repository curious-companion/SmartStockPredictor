{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23a698bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'NSE-Data'...\n",
      "Updating files:   0% (6/2372)\n",
      "Updating files:   1% (24/2372)\n",
      "Updating files:   2% (48/2372)\n",
      "Updating files:   3% (72/2372)\n",
      "Updating files:   4% (95/2372)\n",
      "Updating files:   5% (119/2372)\n",
      "Updating files:   6% (143/2372)\n",
      "Updating files:   7% (167/2372)\n",
      "Updating files:   8% (190/2372)\n",
      "Updating files:   9% (214/2372)\n",
      "Updating files:   9% (226/2372)\n",
      "Updating files:  10% (238/2372)\n",
      "Updating files:  11% (261/2372)\n",
      "Updating files:  12% (285/2372)\n",
      "Updating files:  13% (309/2372)\n",
      "Updating files:  14% (333/2372)\n",
      "Updating files:  15% (356/2372)\n",
      "Updating files:  16% (380/2372)\n",
      "Updating files:  17% (404/2372)\n",
      "Updating files:  18% (427/2372)\n",
      "Updating files:  19% (451/2372)\n",
      "Updating files:  19% (458/2372)\n",
      "Updating files:  20% (475/2372)\n",
      "Updating files:  21% (499/2372)\n",
      "Updating files:  22% (522/2372)\n",
      "Updating files:  23% (546/2372)\n",
      "Updating files:  24% (570/2372)\n",
      "Updating files:  25% (593/2372)\n",
      "Updating files:  26% (617/2372)\n",
      "Updating files:  27% (641/2372)\n",
      "Updating files:  27% (664/2372)\n",
      "Updating files:  28% (665/2372)\n",
      "Updating files:  29% (688/2372)\n",
      "Updating files:  30% (712/2372)\n",
      "Updating files:  31% (736/2372)\n",
      "Updating files:  32% (760/2372)\n",
      "Updating files:  33% (783/2372)\n",
      "Updating files:  34% (807/2372)\n",
      "Updating files:  35% (831/2372)\n",
      "Updating files:  36% (854/2372)\n",
      "Updating files:  37% (878/2372)\n",
      "Updating files:  37% (890/2372)\n",
      "Updating files:  38% (902/2372)\n",
      "Updating files:  39% (926/2372)\n",
      "Updating files:  40% (949/2372)\n",
      "Updating files:  41% (973/2372)\n",
      "Updating files:  42% (997/2372)\n",
      "Updating files:  43% (1020/2372)\n",
      "Updating files:  44% (1044/2372)\n",
      "Updating files:  45% (1068/2372)\n",
      "Updating files:  46% (1092/2372)\n",
      "Updating files:  47% (1115/2372)\n",
      "Updating files:  48% (1139/2372)\n",
      "Updating files:  48% (1146/2372)\n",
      "Updating files:  49% (1163/2372)\n",
      "Updating files:  50% (1186/2372)\n",
      "Updating files:  51% (1210/2372)\n",
      "Updating files:  52% (1234/2372)\n",
      "Updating files:  53% (1258/2372)\n",
      "Updating files:  54% (1281/2372)\n",
      "Updating files:  55% (1305/2372)\n",
      "Updating files:  56% (1329/2372)\n",
      "Updating files:  57% (1353/2372)\n",
      "Updating files:  58% (1376/2372)\n",
      "Updating files:  59% (1400/2372)\n",
      "Updating files:  60% (1424/2372)\n",
      "Updating files:  61% (1447/2372)\n",
      "Updating files:  61% (1463/2372)\n",
      "Updating files:  62% (1471/2372)\n",
      "Updating files:  63% (1495/2372)\n",
      "Updating files:  64% (1519/2372)\n",
      "Updating files:  65% (1542/2372)\n",
      "Updating files:  66% (1566/2372)\n",
      "Updating files:  67% (1590/2372)\n",
      "Updating files:  68% (1613/2372)\n",
      "Updating files:  69% (1637/2372)\n",
      "Updating files:  70% (1661/2372)\n",
      "Updating files:  71% (1685/2372)\n",
      "Updating files:  71% (1697/2372)\n",
      "Updating files:  72% (1708/2372)\n",
      "Updating files:  73% (1732/2372)\n",
      "Updating files:  74% (1756/2372)\n",
      "Updating files:  75% (1779/2372)\n",
      "Updating files:  76% (1803/2372)\n",
      "Updating files:  77% (1827/2372)\n",
      "Updating files:  78% (1851/2372)\n",
      "Updating files:  79% (1874/2372)\n",
      "Updating files:  80% (1898/2372)\n",
      "Updating files:  81% (1922/2372)\n",
      "Updating files:  82% (1946/2372)\n",
      "Updating files:  82% (1952/2372)\n",
      "Updating files:  83% (1969/2372)\n",
      "Updating files:  84% (1993/2372)\n",
      "Updating files:  85% (2017/2372)\n",
      "Updating files:  86% (2040/2372)\n",
      "Updating files:  87% (2064/2372)\n",
      "Updating files:  88% (2088/2372)\n",
      "Updating files:  89% (2112/2372)\n",
      "Updating files:  90% (2135/2372)\n",
      "Updating files:  91% (2159/2372)\n",
      "Updating files:  92% (2183/2372)\n",
      "Updating files:  92% (2200/2372)\n",
      "Updating files:  93% (2206/2372)\n",
      "Updating files:  94% (2230/2372)\n",
      "Updating files:  94% (2244/2372)\n",
      "Updating files:  95% (2254/2372)\n",
      "Updating files:  95% (2263/2372)\n",
      "Updating files:  95% (2273/2372)\n",
      "Updating files:  96% (2278/2372)\n",
      "Updating files:  96% (2280/2372)\n",
      "Updating files:  96% (2282/2372)\n",
      "Updating files:  96% (2283/2372)\n",
      "Updating files:  96% (2285/2372)\n",
      "Updating files:  96% (2287/2372)\n",
      "Updating files:  96% (2288/2372)\n",
      "Updating files:  96% (2289/2372)\n",
      "Updating files:  96% (2291/2372)\n",
      "Updating files:  96% (2293/2372)\n",
      "Updating files:  96% (2295/2372)\n",
      "Updating files:  96% (2296/2372)\n",
      "Updating files:  96% (2297/2372)\n",
      "Updating files:  96% (2299/2372)\n",
      "Updating files:  96% (2300/2372)\n",
      "Updating files:  97% (2301/2372)\n",
      "Updating files:  97% (2303/2372)\n",
      "Updating files:  97% (2304/2372)\n",
      "Updating files:  97% (2306/2372)\n",
      "Updating files:  97% (2307/2372)\n",
      "Updating files:  97% (2308/2372)\n",
      "Updating files:  97% (2309/2372)\n",
      "Updating files:  97% (2310/2372)\n",
      "Updating files:  97% (2311/2372)\n",
      "Updating files:  97% (2312/2372)\n",
      "Updating files:  97% (2313/2372)\n",
      "Updating files:  97% (2314/2372)\n",
      "Updating files:  97% (2315/2372)\n",
      "Updating files:  97% (2316/2372)\n",
      "Updating files:  97% (2317/2372)\n",
      "Updating files:  97% (2318/2372)\n",
      "Updating files:  97% (2320/2372)\n",
      "Updating files:  97% (2321/2372)\n",
      "Updating files:  98% (2325/2372)\n",
      "Updating files:  98% (2328/2372)\n",
      "Updating files:  98% (2337/2372)\n",
      "Updating files:  98% (2345/2372)\n",
      "Updating files:  99% (2349/2372)\n",
      "Updating files:  99% (2355/2372)\n",
      "Updating files:  99% (2364/2372)\n",
      "Updating files: 100% (2372/2372)\n",
      "Updating files: 100% (2372/2372), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ShabbirHasan1/NSE-Data.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "293b257a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All stock and index data combined and saved to: all_stock_and_index_data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Base directories (adjust if needed)\n",
    "stock_base_dir = \"NSE-Data/NSE Minute Data/NSE_Stocks_Data\"\n",
    "index_base_dir = \"NSE-Data/NSE Minute Data/NSE_Index_Data\"\n",
    "\n",
    "# All stock files\n",
    "stock_files = [\n",
    "    \"RELIANCE__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"HDFCBANK__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"AXISBANK__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"ADANIENT__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"ADANIPORTS__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"INFY__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"SBIN__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"ONGC__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"BHARTIARTL__EQ__NSE__NSE__MINUTE.csv\",\n",
    "    \"BAJFINANCE__EQ__NSE__NSE__MINUTE.csv\"\n",
    "]\n",
    "\n",
    "# All index files\n",
    "index_files = [\n",
    "    \"NIFTY_BANK__EQ__INDICES__NSE__MINUTE.csv\",\n",
    "    \"NIFTY_50__EQ__INDICES__NSE__MINUTE.csv\",\n",
    "    \"NIFTY_MIDCAP_100__EQ__INDICES__NSE__MINUTE.csv\",\n",
    "    \"NIFTY_AUTO__EQ__INDICES__NSE__MINUTE.csv\",\n",
    "    \"NIFTY_FIN_SERVICE__EQ__INDICES__NSE__MINUTE.csv\"\n",
    "]\n",
    "\n",
    "# Construct full paths\n",
    "stock_paths = [os.path.join(stock_base_dir, f) for f in stock_files]\n",
    "index_paths = [os.path.join(index_base_dir, f) for f in index_files]\n",
    "\n",
    "# Output file (saved to local directory)\n",
    "output_file = \"all_stock_and_index_data.csv\"\n",
    "\n",
    "# Delete output file if it already exists\n",
    "if os.path.exists(output_file):\n",
    "    os.remove(output_file)\n",
    "\n",
    "# Function to combine files\n",
    "def combine_csv_files(file_paths, output_file):\n",
    "    header_written = False\n",
    "    for file_path in file_paths:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"❌ File not found: {file_path}\")\n",
    "            continue\n",
    "        try:\n",
    "            for chunk in pd.read_csv(file_path, chunksize=50000):\n",
    "                chunk.to_csv(output_file, mode='a', index=False, header=not header_written)\n",
    "                header_written = True\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file_path}: {e}\")\n",
    "\n",
    "# Combine stock and index CSVs\n",
    "combine_csv_files(stock_paths + index_paths, output_file)\n",
    "\n",
    "print(\"✅ All stock and index data combined and saved to:\", output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "510fba5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         timestamp      open      high       low     close  \\\n",
      "5364196  2021-01-01 15:25:00+05:30  15190.35  15190.35  15175.90  15181.50   \n",
      "5364197  2021-01-01 15:26:00+05:30  15182.85  15183.05  15173.00  15177.00   \n",
      "5364198  2021-01-01 15:27:00+05:30  15176.15  15180.40  15175.35  15178.95   \n",
      "5364199  2021-01-01 15:28:00+05:30  15176.65  15179.65  15171.50  15174.35   \n",
      "5364200  2021-01-01 15:29:00+05:30  15173.35  15178.65  15168.25  15174.35   \n",
      "\n",
      "         volume  \n",
      "5364196     0.0  \n",
      "5364197     0.0  \n",
      "5364198     0.0  \n",
      "5364199     0.0  \n",
      "5364200     0.0  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('all_stock_and_index_data.csv')\n",
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eedb2317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp    0\n",
      "open         0\n",
      "high         0\n",
      "low          0\n",
      "close        0\n",
      "volume       0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b040791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.ffill(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1835fa29",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, 'timestamp'] = pd.to_datetime(df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9849f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['timestamp'] = pd.to_datetime(df['timestamp'])  # Convert to datetime format\n",
    "\n",
    "df.loc[:, 'hour'] = df['timestamp'].dt.hour\n",
    "df.loc[:, 'day_of_week'] = df['timestamp'].dt.dayofweek\n",
    "df.loc[:, 'month'] = df['timestamp'].dt.month\n",
    "df.loc[:, 'year'] = df['timestamp'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d2f057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  timestamp    open    high     low   close   volume  hour  \\\n",
      "0 2017-01-02 09:15:00+05:30  536.55  537.85  535.60  535.90  30989.0     9   \n",
      "1 2017-01-02 09:16:00+05:30  536.35  536.65  536.05  536.40  15076.0     9   \n",
      "2 2017-01-02 09:17:00+05:30  536.55  536.55  532.20  532.20  53438.0     9   \n",
      "3 2017-01-02 09:18:00+05:30  532.30  534.85  531.15  534.85  35025.0     9   \n",
      "4 2017-01-02 09:19:00+05:30  534.90  535.10  534.35  534.80  21102.0     9   \n",
      "\n",
      "   day_of_week  month  year  \n",
      "0            0      1  2017  \n",
      "1            0      1  2017  \n",
      "2            0      1  2017  \n",
      "3            0      1  2017  \n",
      "4            0      1  2017  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de37cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['timestamp'].dt.time >= pd.to_datetime(\"09:15:00\").time()) &\n",
    "        (df['timestamp'].dt.time <= pd.to_datetime(\"15:30:00\").time())]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a623848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler_close_price.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Fit and transform the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(df[['close']].values)\n",
    "\n",
    "# Save the scaler to a file\n",
    "joblib.dump(scaler, 'scaler_close_price.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01f72c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_sequences(data, time_steps=60):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X.append(data[i:i + time_steps])\n",
    "        y.append(data[i + time_steps])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences(df[['close']].values)  # Use 'Close' prices for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257e24a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature and target scalers saved.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Feature Engineering\n",
    "df['SMA_10'] = df['close'].rolling(window=10).mean()\n",
    "df['EMA_9'] = df['close'].ewm(span=9, adjust=False).mean()\n",
    "df['EMA_21'] = df['close'].ewm(span=21, adjust=False).mean()\n",
    "\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = np.where(delta > 0, delta, 0)\n",
    "    loss = np.where(delta < 0, -delta, 0)\n",
    "    avg_gain = pd.Series(gain, index=data.index).rolling(window=window).mean()\n",
    "    avg_loss = pd.Series(loss, index=data.index).rolling(window=window).mean()\n",
    "    rs = np.where(avg_loss == 0, 100, avg_gain / avg_loss)\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return pd.Series(rsi, index=data.index)\n",
    "\n",
    "df['RSI_14'] = compute_rsi(df['close'], 14)\n",
    "\n",
    "df['MA_20'] = df['close'].rolling(window=20).mean()\n",
    "rolling_std = df['close'].rolling(window=20).std()\n",
    "df['BB_Upper'] = df['MA_20'] + 2 * rolling_std\n",
    "df['BB_Lower'] = df['MA_20'] - 2 * rolling_std\n",
    "\n",
    "df['EMA_12'] = df['close'].ewm(span=12, adjust=False).mean()\n",
    "df['EMA_26'] = df['close'].ewm(span=26, adjust=False).mean()\n",
    "df['MACD'] = df['EMA_12'] - df['EMA_26']\n",
    "df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "df['L14'] = df['low'].rolling(window=14).min()\n",
    "df['H14'] = df['high'].rolling(window=14).max()\n",
    "df['%K'] = df['close'] - df['L14']\n",
    "df['%D'] = df['%K'].rolling(window=3).mean()\n",
    "\n",
    "df['HL'] = df['high'] - df['low']\n",
    "df['HC'] = abs(df['high'] - df['close'].shift())\n",
    "df['LC'] = abs(df['low'] - df['close'].shift())\n",
    "df['TR'] = df[['HL', 'HC', 'LC']].max(axis=1)\n",
    "df['ATR_14'] = df['TR'].rolling(window=14).mean()\n",
    "\n",
    "# Drop intermediate columns to avoid leakage\n",
    "df.drop(columns=['HL', 'HC', 'LC', 'L14', 'H14', 'TR'], inplace=True)\n",
    "\n",
    "# Drop rows with NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Define features and target\n",
    "features = ['open', 'high', 'low', 'volume', 'SMA_10', 'EMA_9', 'EMA_21', 'RSI_14',\n",
    "            'MA_20', 'BB_Upper', 'BB_Lower', 'EMA_12', 'EMA_26',\n",
    "            'MACD', 'Signal_Line', '%K', '%D', 'ATR_14']\n",
    "target = ['close']\n",
    "\n",
    "# Ensure all features and target are numeric\n",
    "df[features + target] = df[features + target].apply(pd.to_numeric)\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "target_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit-transform features and target\n",
    "features_scaled = feature_scaler.fit_transform(df[features])\n",
    "target_scaled = target_scaler.fit_transform(df[target])\n",
    "\n",
    "# Assign back to DataFrame\n",
    "df[features] = features_scaled\n",
    "df[target] = target_scaled\n",
    "\n",
    "# Save scalers\n",
    "joblib.dump(feature_scaler, 'feature_scaler.pkl')\n",
    "joblib.dump(target_scaler, 'target_scaler.pkl')\n",
    "\n",
    "# Save preprocessed data\n",
    "df.to_csv('preprocessed_data.csv', index=False)\n",
    "\n",
    "print(\"✅ Feature and target scalers saved. Data scaled between 0 and 1.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
